{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## BEGINING WIKIHATE DATASET ##\n",
    "# wiki = pd.read_csv('data/wiki/toxicity_annotated_comments.tsv', sep = '\\t')\n",
    "# wiki_labels = pd.read_csv('data/wiki/toxicity_annotations.tsv', sep = '\\t')\n",
    "#\n",
    "# print(len(wiki_labels))\n",
    "#\n",
    "# # remove NEWLINE_TOKEN and TAB_TOKEN from wiki comments\n",
    "# wiki['comment'] = wiki['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "# wiki['comment'] = wiki['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "#\n",
    "# wiki_labels = wiki_labels.drop(['worker_id', 'toxicity'], axis = 1)\n",
    "# wiki = wiki.drop(['year', 'logged_in', 'ns', 'sample', 'split'], axis = 1)\n",
    "# wiki = wiki.dropna()\n",
    "# wiki_labels = wiki_labels.dropna()\n",
    "# wiki = wiki.merge(wiki_labels, on = 'rev_id')\n",
    "#\n",
    "# print(wiki.head(5)['comment'])\n",
    "# wiki = wiki.drop(['rev_id'], axis = 1)\n",
    "#\n",
    "# train, test = train_test_split(wiki, test_size = 0.2, random_state = 42)\n",
    "# train, val = train_test_split(train, test_size = 0.25, random_state = 42)\n",
    "#\n",
    "# print(train.head(5))\n",
    "# print(val.head(5))\n",
    "# print(test.head(5))\n",
    "#\n",
    "# print(train.shape, val.shape, test.shape)\n",
    "#\n",
    "# train.to_csv('data/wiki/train.csv', index = False)\n",
    "# val.to_csv('data/wiki/val.csv', index = False)\n",
    "# test.to_csv('data/wiki/test.csv', index = False)\n",
    "#\n",
    "\n",
    "# with zipfile.ZipFile('data/wiki/wiki.zip', 'w') as z:\n",
    "#     z.write('data/wiki/train.csv')\n",
    "#     z.write('data/wiki/val.csv')\n",
    "#     z.write('data/wiki/test.csv')\n",
    "\n",
    "## END WIKIHATE DATASET ##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghait\\AppData\\Local\\Temp\\ipykernel_19764\\711295243.py:3: DtypeWarning: Columns (0,4,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hateData = pd.read_csv('../../Twitterdatasets/Toraman22_hate_speech_v1/Toraman22_hate_speech_v1.tsv', sep='\\t', )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "113598\n",
      "       label                                               text  label_0  \\\n",
      "40053      0  You know maybe doing a “challenge” where I dri...        1   \n",
      "40054      0  RT @thehill: Black transgender woman found dea...        1   \n",
      "40055      0  2021 Goals:\\nPlaytest and release Rumrunners.\\...        1   \n",
      "40056      1  The way akimi yoshida basically killed off mos...        1   \n",
      "40057      1  just got reminded of the time that when i was ...        1   \n",
      "\n",
      "       label_1  label_2  \n",
      "40053        1        0  \n",
      "40054        0        0  \n",
      "40055        0        1  \n",
      "40056        1        1  \n",
      "40057        1        1  \n",
      "(68158, 5) (22720, 5) (22720, 5)\n"
     ]
    }
   ],
   "source": [
    "## BEGINNING HATE DATASET ##\n",
    "\n",
    "hateData = pd.read_csv('../../Twitterdatasets/Toraman22_hate_speech_v1/Toraman22_hate_speech_v1.tsv', sep='\\t', )\n",
    "\n",
    "hateData = hateData.drop(hateData[hateData['language'] != 1].index)\n",
    "\n",
    "print(len(hateData[hateData['language'] == 0]))\n",
    "\n",
    "hateData = hateData.drop(\n",
    "    ['user_name', 'screen_name', 'verified', 'user_id', 'created_at', 'friends_count', 'followers_count',\n",
    "     'statuses_count', 'favourites_count', 'default_pic', 'date', 'tweet_id', 'topic', 'label_score', 'language'],\n",
    "    axis=1)\n",
    "\n",
    "print(len(hateData))\n",
    "\n",
    "hateData['label'] = hateData['label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "for i in range(3):\n",
    "    col = 'label_' + str(i)\n",
    "    hateData[col] = hateData[col].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "print(hateData.head(5))\n",
    "\n",
    "train, test = train_test_split(hateData, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(train.shape, val.shape, test.shape)\n",
    "\n",
    "train.to_csv('../../Twitterdatasets/train.csv', index=False)\n",
    "test.to_csv('../../Twitterdatasets/test.csv', index=False)\n",
    "val.to_csv('../../Twitterdatasets/val.csv', index=False)\n",
    "\n",
    "# with zipfile.ZipFile('../../Twitterdatasets/hate.zip', 'w') as z:\n",
    "#     z.write('../../Twitterdatasets/train.csv')\n",
    "#     z.write('../../Twitterdatasets/val.csv')\n",
    "#     z.write('../../Twitterdatasets/test.csv')\n",
    "## END HATE DATASET ##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
